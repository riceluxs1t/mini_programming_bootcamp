{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 000 \n",
    "## Introduction to Data Science Using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bio : Won Suh (서 원), won782@gmail.com\n",
    "\n",
    "Rice Class of 2014 Statistics\n",
    "\n",
    "I currently work at [Panorama Education](http://www.panoramaed.com) as a data scientist and live in Boston, MA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda : 간단한 데이터 사이언스 소개와 예제\n",
    "* What is Data Science?\n",
    "* List Comprehension / Lambda Functions\n",
    "* Toy Example : Predicting Housing Price in Boston\n",
    "* Introduction to Word2vec\n",
    "* (Assignment) Constructing Word2vec Operator Class\n",
    "* References / Online Materials to Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Data Science?\n",
    "\n",
    "데이터 사이언스는 최근 5~10년간 생겨난 복합적 분야로 수학 / 통계학 / 컴퓨터 공학 / 전자전기 공학 등 기초 과학과 컴퓨터 프로그램을 응용하여, 데이터를 활용하여 부가가치를 만들어내는 분야입니다.\n",
    "\n",
    "전통적인 통계학에서의 회귀분석등의 단순 수학적 모델을 넘어 컴퓨터 공학과 전자전기 공학등을 응용한 인공신경망 / 딥러닝등의 등장으로 많은 각광을 받고 있습니다.\n",
    "\n",
    "예측 모델은 만드는것 뿐만 아니라, 데이터 분석, 데이터 시각화등을 활용하여, 비전공자들이나 일반 사용자들에게 쉽게 설명하는것도 일부분을 차지합니다.\n",
    "\n",
    "토론토 대학의 [Geoffrey Hinton](http://www.cs.toronto.edu/~hinton/) 와 그 랩의 학생들이 꾸준이 연구하여 최근 수년간 딥러닝을 상용화 하였으며, 아래의 분야에서 상용화 되거나 상용화 가능한 수준으로 올라섰습니다.\n",
    "* OCR (Object Character Recognition)\n",
    "* Neural Machine Translation (Quoc V. Le, Google)\n",
    "* Image Classification (AlexNet)\n",
    "* Reinforcement Learning in Games (AlphaGo and others)\n",
    "* Video / Image Sequence Prediction (Generative Adversarial Networks)\n",
    "* Natural Language Processing and Word Sementics (Word2vec, ConceptNet ...)\n",
    "\n",
    "위의 연구분야는 서로 융합하여 아마존의 Alexa, 테슬라의 Self-driving car, 구글의 AlphaGo 등을 가능하게 합니다.\n",
    "\n",
    "이번 렉쳐에서는 아주 간단한 예제와 함께, Word2vec 을 응용한 사전은 만들어 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List Comprehension / Lambda Functions\n",
    "\n",
    "우선 데이터 사이언스의 가장 중요한 작업은 데이터를 분석에 가능한 형태로 만드는 것입니다. 통계에 따르면 데이터 사이언티스트들은 평균적으로 80%의 시간을 데이터 클리닝 / wrangling 에 사용하고 있으며, 저 역시 예외는 아닙니다.\n",
    "\n",
    "데이터베이스에 있는 (관계형 데이터베이스) 깔끔한 데이터도 있는 반면, 텍스트, json, csv 등 다양한 데이터 형태가 존재하고, 양식 혹은 단위 등도 다르기에, 데이터를 깔끔하게 정리하고 합치는 과정이 매우 매우 중요합니다. 실제로 이 과정을 얼만큼 효율적으로 깔끔하게 처리하는지가 데이터 사이언티스트의 능력을 가늠하기도 합니다 (80%의 시간을 할애하는 과정을 오차없이 빨리 처리할수 있다면, 매우 유리합니다).\n",
    "\n",
    "파이썬은 R 과 더불어 데이터 사이언스의 가장 중요한 툴입니다.\n",
    "\n",
    "본 렉쳐에서는 간단한 데이터 정리법을 소개하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wsuh/pyVenv/venv/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Toy Example : Predicting Housing Price in Boston\n",
    "\n",
    "scikit-learn 에서 기본적으로 제공되는 boston 데이터를 활용하여 보스턴의 집값을 예측하는 선형함수를 만들어보겠습니다.\n",
    "\n",
    "(Optional Assignment) 본 렉쳐에 있는 코드는 작동되는 코드입니다 (수정하지 않아도 답을 제출합니다). 비선형 함수 혹은 다른 알고리즘을 활용하여, 본 렉쳐에서 제공된 오차 (Root Mean Squared Error) 를 줄여보십시오. 그리고 그 후, 왜 오차가 줄어 들었는지, 오차가 줄면 줄수록 좋을지 생각해보십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sklearn.datasets.load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introduction to Word2vec\n",
    "\n",
    "위의 소개에서 열거된 데이터사이언스의 활용분야중 가장 까다롭고 아직 많은 도전 과제를 남긴 분야는 자연어 처리 (natural langugage processing) 입니다. Recurrent Neural Network 등 다양한 방법들이 활용되고 있지만, 컴퓨터가 언어를 이해한다는 것은 매우 어려운 과제로 여겨지고 있습니다.\n",
    "\n",
    "수년전, 이분야에서 매우 promising 한 연구가 있었고 이는 Word2vec 입니다.\n",
    "\n",
    "말 그대로 단어를 벡터공간에 프로젝트 하는것인데, 이는 단어의 사칙연산을 가능하게 합니다.\n",
    "\n",
    "가령, 왕 - 남자 + 여자 = 여왕 같은 단어의 추상적 의미를 이용한 사칙연산을 할수 있으며, 파리 : 프랑스 = x : 영국 은 런던을 return 하는 등, 단어의 관계적 의미 또한 나타낼수 있습니다.\n",
    "\n",
    "이는 사전 혹은 단어간의 관계를 나타낸 graph 를 활용하지 않고, 단어의 벡터값만 가지고 가능한 연산입니다.\n",
    "\n",
    "흔히 100 또는 300차원의 벡터 공간을 사용하며, 위키피디아 / 웹데이터 등으로 (훌륭하게) 훈련된 벡터값을 오픈소스로 쉽게 구할수 있습니다.\n",
    "\n",
    "### Training Word2vec\n",
    "그렇다면 단어를 어떻게 벡터값으로 변환할수 있을까요? 본 렉쳐에서는 훈련과정은 생략하나 간단한 설명을 하겠습니다. (훈련은 Tensorflow 등 라이브러리로 그다지 어렵지 않게 가능합니다. 다만 엄청난 양의 데이터를 필요로 하며, 로컬에서 훈련하는것은 컴퓨터의 수명 / 성능을 위해 비추천합니다)\n",
    "\n",
    "![Word2Vec in Tensorflow](https://www.tensorflow.org/versions/r0.11/images/softmax-nplm.png)\n",
    "\n",
    "위 다이어그램은 Word2vec 트레이닝 모델중 하나인 CBOW - continuous bag of words (opposed to skip-gram) 입니다. 주어진 문장과 타겟 단어 (위 경우에는 매트)를 준비한후, 주변 단어와 주변 단어의 벡터 값을 이용하여 타켓 단어를 예측하는 인공신경망을 만듭니다. 인공신경망은 그 후 타켓 단어를 예측했는지, 못했는지를 확인한후 이에 따라 발생하는 에러를 넘겨 주변 단어의 벡터값을 조금씩 수정합니다. 이러한 과정을 수천만번 이상 (기가~페타바이트 단위의 텍스르) 거치면, 단어의 벡터는 주변단어와 타겟단어의 배치에 따라 발생하는 단어의 추상적 의미를 반영하게 됩니다.\n",
    "\n",
    "### Using Open Source Word2vec\n",
    "위와 같은 학습과정은 알고보면 매우 단순합니다. 인공신경망에 대한 간단한 이해, softmax, backpropagation, gradient descent, delta rule 에 대한 이해만 있으면 스스로 코드를 짤수도 있습니다. 허나, 필요한 데이터 양과 학습에 필요한 시간이 많이 요구되어 본 렉쳐에서는 커버하지 않겠습니다.\n",
    "\n",
    "이러한 학습을 거친 오픈소스 파일들이 많이 있는데, 여기에서는 Glove 라는 Stanford NLP 에서 공개된 파일을 사용하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Assignment) Constructing Word2vec Operator Class\n",
    "\n",
    "숙제로는 **fun_word** 클라스를 만들어 보겠습니다.\n",
    "\n",
    "1. load_data 는 Glove 데이터를 로드하여 words, vectors 에 저장합니다.\n",
    "2. vector_lookup(word) 는 해당 단어의 벡터값을 리턴합니다.\n",
    "3. closest_n(word, n) 은 해당 단어와 가장 유사한 n 개의 단어를 리턴합니다.\n",
    "4. word_relation(n1, n2, w1) 은 위의 런던과 같은 비례식의 값을 리턴합니다.\n",
    "5. word_operation([list of words], [list of operators; + or -]) 는 제공된 리스트의 단어 (n개) 간의 n-1개 의 연산을 처리하고, 가장 결과에 가까운 단어를 리턴합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fun_word(directory) :\n",
    "    def __self__ :\n",
    "        self.directory = directory\n",
    "        self.vocab, self.vector = self.load_data()\n",
    "        \n",
    "    def load_data() :\n",
    "        with open(directory, \"r\") as fp :\n",
    "            vectors_list = []\n",
    "            vocab_lookup = []\n",
    "            for line in fp :\n",
    "                vals = line.rstrip().split(' ')\n",
    "                vocab_lookup.append(vals[0])\n",
    "                vector_list.append(map(float, vals[1:]))\n",
    "        return vocab_lookup, vector_list\n",
    "    \n",
    "    def vector_lookup(word) :\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def closest_n(word, n) :\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def word_relation(word, n) :\n",
    "        \n",
    "        return ''\n",
    "    \n",
    "    def word_operation(words, operators) :\n",
    "        \n",
    "        return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References / Online Materials to Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
